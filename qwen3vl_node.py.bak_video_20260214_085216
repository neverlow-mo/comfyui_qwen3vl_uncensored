import os
import tempfile
from PIL import Image

from .llama_wrapper import LlamaWrapper


PRESET_CHOICES = [
    "üñºÔ∏è Tags",
    "üñºÔ∏è Simple Description",
    "üñºÔ∏è Detailed Description",
    "üñºÔ∏è Ultra Detailed Description",
    "üé¨ Cinematic Description",
    "üñºÔ∏è Detailed Analysis",
    "üìπ Video Summary",
    "üìñ Short Story",
    "ü™Ñ Prompt Refine & Expand",
]


class Qwen3VLPromptGenerator:
    """
    ComfyUI Node: Qwen3-VL Prompt Generator (Uncensored)
    Modes:
      - enhance: text -> refined prompt
      - describe: image -> description/prompt
      - multimodal: text+image -> prompt
    """

    def __init__(self):
        self.llama = None  # lazy init

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mode": (["enhance", "describe", "multimodal"], {"default": "enhance"}),
                "text": ("STRING", {"default": "", "multiline": True}),
                "image": ("IMAGE",),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.05}),
                "max_tokens": ("INT", {"default": 400, "min": 16, "max": 4096}),
                "preset_prompt": (PRESET_CHOICES, {"default": "üñºÔ∏è Detailed Analysis"}),
                "use_preset_tokens": ("BOOLEAN", {"default": True}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 2147483647}),
                "control_after_generate": (["fixed", "increment", "decrement", "randomize"], {"default": "randomize"}),
                "keep_model_loaded": ("BOOLEAN", {"default": True}),
            }
        }

    RETURN_TYPES = ("STRING", "INT")
    RETURN_NAMES = ("generated_prompt", "seed")
    FUNCTION = "generate_prompt"
    CATEGORY = "prompt/qwen3vl"

    def _ensure_llama(self):
        if self.llama is None:
            print("[Qwen3-VL] ‚úÖ LlamaWrapper initialisiert")
            self.llama = LlamaWrapper()

    def _save_image_tmp(self, image_tensor) -> str:
        """
        ComfyUI IMAGE tensor -> tmp png path
        """
        # image_tensor is typically a torch tensor or numpy-like; Comfy returns [B,H,W,C] float 0..1
        import numpy as np

        arr = image_tensor
        if hasattr(arr, "cpu"):
            arr = arr.cpu().numpy()
        arr = np.asarray(arr)
        if arr.ndim == 4:
            arr = arr[0]
        arr = (arr * 255.0).clip(0, 255).astype("uint8")

        img = Image.fromarray(arr)
        fd, path = tempfile.mkstemp(suffix=".png")
        os.close(fd)
        img.save(path)
        return path

    def _next_seed(self, seed: int, cafg: str) -> int:
        cafg = (cafg or "randomize").strip().lower()
        if cafg == "fixed":
            return seed
        if cafg == "increment":
            return seed + 1
        if cafg == "decrement":
            return max(0, seed - 1)
        # randomize
        import random
        return random.randint(0, 2147483647)

    def generate_prompt(
        self,
        mode: str,
        text: str = "",
        image=None,
        temperature: float = 0.7,
        max_tokens: int = 400,
        preset_prompt: str = "üñºÔ∏è Detailed Analysis",
        use_preset_tokens: bool = True,
        seed: int = 0,
        control_after_generate: str = "randomize",
        keep_model_loaded: bool = True,
        **kwargs,
    ):
        """
        Main entry used by ComfyUI
        """
        try:
            self._ensure_llama()

            seed = int(seed or 0)
            next_seed = self._next_seed(seed, control_after_generate)

            preset_prompt = (preset_prompt or "üñºÔ∏è Detailed Analysis").strip()
            use_preset_tokens = bool(use_preset_tokens)
            keep_model_loaded = bool(keep_model_loaded)
            temperature = float(temperature)
            max_tokens = int(max_tokens)

            # NOTE:
            # - max_tokens is only used when use_preset_tokens=False (manual cap)
            # - when use_preset_tokens=True, wrapper uses preset token defaults
            #   and ignores max_tokens to match QwenVL GGUF behavior.
            if not use_preset_tokens:
                # override wrapper default by telling it not to use preset tokens
                # (wrapper will pass max_tokens as n_predict via its own logic if you implement it;
                #  for now we pass use_preset_tokens=False and wrapper keeps n_predict None,
                #  but server branch uses max_tokens directly, so that's where it matters.)
                pass

            if mode == "enhance":
                print(f"[Qwen3-VL] Enhancing text: '{(text or '')[:32]}...'")
                result = self.llama.enhance_prompt(
                    text or "",
                    preset_prompt=preset_prompt,
                    use_preset_tokens=use_preset_tokens,
                    seed=seed,
                    keep_model_loaded=keep_model_loaded,
                    temperature=temperature,
                )

            elif mode == "describe":
                if image is None:
                    return ("‚ùå ERROR: no image provided for describe mode", next_seed)
                img_path = self._save_image_tmp(image)
                try:
                    print(f"[Qwen3-VL] Describing image: {img_path}")
                    result = self.llama.describe_image(
                        img_path,
                        preset_prompt=preset_prompt,
                        use_preset_tokens=use_preset_tokens,
                        seed=seed,
                        keep_model_loaded=keep_model_loaded,
                        temperature=temperature,
                    )
                finally:
                    try:
                        os.remove(img_path)
                    except Exception:
                        pass

            else:  # multimodal
                if image is None:
                    return ("‚ùå ERROR: no image provided for multimodal mode", next_seed)
                img_path = self._save_image_tmp(image)
                try:
                    print(f"[Qwen3-VL] Multimodal: text='{(text or '')[:32]}...' + image")
                    result = self.llama.multimodal_enhance(
                        text or "",
                        img_path,
                        preset_prompt=preset_prompt,
                        use_preset_tokens=use_preset_tokens,
                        seed=seed,
                        keep_model_loaded=keep_model_loaded,
                        temperature=temperature,
                    )
                finally:
                    try:
                        os.remove(img_path)
                    except Exception:
                        pass

            result = (result or "").strip()
            print(f"[Qwen3-VL] Generated prompt ({len(result)} chars)")
            return (result, next_seed)

        except Exception as e:
            print(f"[Qwen3-VL] ‚ùå ERROR in {mode} mode:\n{e}")
            return (f"‚ùå ERROR: {e}", int(seed or 0))


NODE_CLASS_MAPPINGS = {
    "Qwen3VLPromptGenerator": Qwen3VLPromptGenerator
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VLPromptGenerator": "Qwen3-VL Prompt Generator (Uncensored) üîì"
}
