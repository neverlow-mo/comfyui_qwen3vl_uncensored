import os
import re
import subprocess
import shlex
import time
from pathlib import Path

# --- Presets (ported from ComfyUI-QwenVL AILab_System_Prompts.json) ---
# PRESET_PROMPTS imported from prompts_config.py
# Token defaults per preset (optional; used when "use_preset_tokens" is enabled)
# PRESET_TOKEN_DEFAULTS imported from prompts_config.py
from . import config
from .prompts_config import PRESET_PROMPTS, PRESET_TOKEN_DEFAULTS, SYSTEM_PROMPTS

from . import server_client
# --- QwenVL(GGUF)-style preset prompts (ported) ---
# PRESET_TOKEN_DEFAULTS imported from prompts_config.py
def build_system_prompt(mode: str, preset: str) -> str:
    # Prefer explicit per-preset system prompt from prompts_config
    sp = SYSTEM_PROMPTS.get(preset)
    if sp:
        return sp

    # Centralized prompts (AILab-style) live in prompts_config.py
    block = SYSTEM_PROMPTS.get(mode, {})
    if not block:
        return "You are a helpful assistant. Output only the final answer."
    return block.get(preset) or block.get("default") or next(iter(block.values()))



class LlamaWrapper:
    """
    llama.cpp wrapper (llama-cli via subprocess)

    Handles:
    - stdout/stderr being inconsistent (some builds print everything to stderr)
    - output written to controlling TTY (/dev/tty), which bypasses PIPE capture

    Strategy:
    1) Try normal PIPE capture (stdout+stderr)
    2) If empty or missing the answer, fall back to controlling PTY (pty.fork)
    3) Extract answer robustly: take block after last \"> \" prompt-echo until \"[ Prompt:\" / \"Exiting...\"
    4) Clean ANSI/control chars/garbage prefixes
    """

    def __init__(self):
        self.llama_cli: Path = config.LLAMA_CLI
        self.model: Path = config.MODEL_PATH
        self.mmproj: Path = config.MMPROJ_PATH

        if not self.llama_cli.exists():
            raise FileNotFoundError("llama-cli nicht gefunden")
        if not self.model.exists():
            raise FileNotFoundError(f"Modell nicht gefunden: {self.model}")

    def _build_cmd(self, full_prompt: str, image_path: str | None = None, n_predict: int | None = None, seed: int | None = None) -> list[str]:
        cmd = [
            str(self.llama_cli),
            "-m", str(self.model),
            "-st",
            "-p", full_prompt,
            "-c", "8192",
            "-ngl", "99",
        ]
        if n_predict is not None:
            cmd.extend(["--n-predict", str(int(n_predict))])

        if seed is not None:
            cmd.extend(["--seed", str(int(seed))])

        if image_path and self.mmproj.exists():
            cmd.extend(["--mmproj", str(self.mmproj), "--image", str(image_path)])
        return cmd

    def _run_pipes(self, cmd: list[str]) -> str:
        result = subprocess.run(cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=180,
            check=True,
        )
        out = (result.stdout or "").strip("\n")
        err = (result.stderr or "").strip("\n")
        return "\n".join([s for s in (out, err) if s]).strip()

    def _run_controlling_pty(self, cmd: list[str]) -> str:
        """
        Run command with a *controlling* PTY using pty.fork().
        Captures output even if the program writes to /dev/tty.
        """
        import pty
        import select

        pid, fd = pty.fork()
        if pid == 0:
            try:
                os.execvp(cmd[0], cmd)
            except Exception:
                os._exit(127)

        chunks: list[bytes] = []
        start = time.time()
        timeout = 180

        try:
            while True:
                if time.time() - start > timeout:
                    try:
                        os.kill(pid, 9)
                    except OSError:
                        pass
                    raise subprocess.TimeoutExpired(cmd, timeout)

                r, _, _ = select.select([fd], [], [], 0.25)
                if r:
                    try:
                        data = os.read(fd, 65536)
                    except OSError:
                        data = b""
                    if data:
                        chunks.append(data)

                # exited?
                try:
                    wpid, _ = os.waitpid(pid, os.WNOHANG)
                except ChildProcessError:
                    wpid = pid

                if wpid == pid:
                    # drain
                    for _ in range(20):
                        r, _, _ = select.select([fd], [], [], 0.05)
                        if not r:
                            break
                        try:
                            data = os.read(fd, 65536)
                        except OSError:
                            break
                        if not data:
                            break
                        chunks.append(data)
                    break

            raw = b"".join(chunks)
            return raw.decode("utf-8", errors="replace").strip()
        finally:
            try:
                os.close(fd)
            except OSError:
                pass


    def _clean_llama_text(self, s: str) -> str:
        # Remove TTY spinner/backspace artifacts like '|\b-\b\\\b \b'
        if not s:
            return s
        s = s.replace("\x08", "")  # backspace
        s = s.replace("\r", "")    # carriage return
        # Strip common spinner residues at the very start
        import re
        s = re.sub(r'^[\|/\\ \-]{2,}', '', s)
        # Strip remaining ASCII control chars (keep newlines intact just in case)
        s = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', s)
        return s

    def _extract_answer(self, text: str) -> str:
        """Extract the model answer from llama.cpp/script output.

        Strategy:
        - Prefer the block after the last 'Output ONLY the ...' line and before '[ Prompt:'.
        - Fallback: last non-empty line that is not meta/stat/log.
        """
        import re

        if not text:
            return ""

        # Normalize line endings and strip ANSI escape codes
        txt = text.replace("\r", "")
        txt = re.sub(r"\x1b\[[0-9;]*[A-Za-z]", "", txt)

        lines = txt.split("\n")

        # Find the last occurrence of the instruction line
        anchor_idx = -1
        for i, line in enumerate(lines):
            if "Output ONLY the" in line:
                anchor_idx = i

        # Find the first stats line after anchor: "[ Prompt:"
        end_idx = len(lines)
        if anchor_idx != -1:
            for j in range(anchor_idx + 1, len(lines)):
                if re.match(r"^\s*\[\s*Prompt\s*:", lines[j], re.IGNORECASE):
                    end_idx = j
                    break

            # Candidate block: between anchor and stats
            block = [l.strip() for l in lines[anchor_idx + 1:end_idx]]
            # Drop empty and obvious meta lines
            cleaned = []
            for l in block:
                if not l:
                    continue
                if l.startswith(("ggml_", "llama_", "build", "model", "modalities", "available commands", "/exit", "/regen", "/clear", "/read", "Loaded media", "Loading model", "Exiting")):
                    continue
                if l.startswith("> "):  # echoed prompt line
                    continue
                if "llama_memory_breakdown_print" in l:
                    continue
                cleaned.append(l)

            ans = "\n".join(cleaned).strip()
            # If we accidentally kept the user prompt, try taking only the paragraph after it
            if ans:
                return ans

        # Fallback: last reasonable non-empty line
        for l in reversed(lines):
            l = l.strip()
            if not l:
                continue
            if re.match(r"^\s*\[\s*Prompt\s*:", l, re.IGNORECASE):
                continue
            if l.startswith(("ggml_", "llama_", "llama_memory_breakdown_print")):
                continue
            if l in ("Exiting...",):
                continue
            if "compute capability" in l:
                continue
            return l

        return ""


    def _run_llama(self, full_prompt: str, image_path: str | None = None, n_predict: int | None = None, seed: int | None = None, keep_model_loaded: bool = False, temperature: float = 0.7, system_prompt: str | None = None) -> str:
        if keep_model_loaded:
            # llama-server path: keeps model loaded & avoids CLI console noise
            return server_client.chat_completions(
                model=self.model.name,
                user=full_prompt,
                image_path=image_path,
                max_tokens=n_predict,
                temperature=temperature,
                seed=seed,
            )

                # If we have a separate system prompt (server-style), merge it for llama-cli fallback
        if system_prompt:
            full_prompt = f"{system_prompt}\n{full_prompt}"

        cmd = self._build_cmd(full_prompt, image_path=image_path, n_predict=n_predict, seed=seed)

        # --- isolate llama output from ComfyUI console spam (FETCH ComfyRegistry etc.) ---
        # Give llama a TTY via `script`, but capture via PIPE so only llama output is captured.
        run_cmd = ["script", "-qfec", shlex.join(cmd), "/dev/null"]
        combined = ""
        try:
            combined = self._run_pipes(run_cmd)
        except subprocess.TimeoutExpired:
            return "âŒ ERROR: llama-cli Timeout (pipes)."
        except subprocess.CalledProcessError as e:
            out = (e.stdout or "").strip()
            err = (e.stderr or "").strip()
            combined = "\n".join([s for s in (out, err) if s]).strip()

        ans = self._extract_answer(combined)
        ans = self._clean_llama_text(ans)

        if not ans:
            try:
                combined = self._run_controlling_pty(cmd)
            except subprocess.TimeoutExpired:
                return "âŒ ERROR: llama-cli Timeout (pty)."
            except Exception as e:
                return f"âŒ ERROR: PTY capture failed: {e}"
            ans = self._extract_answer(combined)
        ans = self._clean_llama_text(ans)

        return ans

    # Public API expected by qwen3vl_node.py
    def enhance_prompt(self, text: str, preset_prompt: str = "ðŸ–¼ï¸ Detailed Analysis", use_preset_tokens: bool = True, seed: int | None = None, keep_model_loaded: bool = False, temperature: float = 0.7) -> str:
        system_prompt = build_system_prompt("enhance", preset_prompt)
        user_prompt = f"User prompt: {text}\nOutput ONLY the final enhanced prompt."
        n_predict = PRESET_TOKEN_DEFAULTS.get(preset_prompt, 700) if use_preset_tokens else None
        return self._run_llama(user_prompt, n_predict=n_predict, seed=seed, keep_model_loaded=keep_model_loaded, temperature=temperature, system_prompt=system_prompt)

    def describe_image(self, image_path: str, preset_prompt: str = "ðŸ–¼ï¸ Detailed Analysis", use_preset_tokens: bool = True, seed: int | None = None) -> str:
        system_prompt = build_system_prompt("describe", preset_prompt)
        user_prompt = ""  # describe uses system prompt only
        n_predict = PRESET_TOKEN_DEFAULTS.get(preset_prompt, 700) if use_preset_tokens else None
        return self._run_llama(user_prompt, image_path=image_path, n_predict=n_predict, seed=seed, system_prompt=system_prompt)

    def multimodal_enhance(self, text: str, image_path: str, preset_prompt: str = "ðŸ–¼ï¸ Detailed Analysis", use_preset_tokens: bool = True, seed: int | None = None) -> str:
        system_prompt = build_system_prompt("multimodal", preset_prompt)
        user_prompt = f"User text: {text}\nOutput ONLY the final enhanced prompt."
        n_predict = PRESET_TOKEN_DEFAULTS.get(preset_prompt, 700) if use_preset_tokens else None
        return self._run_llama(user_prompt, image_path=image_path, n_predict=n_predict, seed=seed, system_prompt=system_prompt)
