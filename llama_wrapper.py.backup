"""
Qwen3-VL Uncensored - llama.cpp Wrapper
========================================
Startet llama-cli als Subprocess und kommuniziert damit

Autor: Martin + Claude Coach
Datum: 2026-02-13
"""

import subprocess
import tempfile
import json
from pathlib import Path
from typing import Optional, Dict, Any

from . import config


class LlamaWrapper:
    """
    Interface zu llama.cpp
    
    Startet llama-cli als Subprocess und sendet Prompts.
    Liest die Antworten und gibt sie zur√ºck.
    """
    
    def __init__(self):
        """Initialisierung"""
        self.llama_cli = config.LLAMA_CLI
        self.model = config.MODEL_PATH
        self.mmproj = config.MMPROJ_PATH
        
        # Validierung
        if not self.llama_cli.exists():
            raise FileNotFoundError(f"llama-cli nicht gefunden: {self.llama_cli}")
        if not self.model.exists():
            raise FileNotFoundError(f"Modell nicht gefunden: {self.model}")
    
    def generate(
        self, 
        prompt: str,
        image_path: Optional[Path] = None,
        system_prompt: Optional[str] = None,
        max_tokens: int = config.MAX_TOKENS,
        temperature: float = config.TEMPERATURE
    ) -> str:
        """
        Generiert eine Antwort vom Modell
        
        Args:
            prompt: Der User-Prompt
            image_path: Optional - Pfad zu einem Bild (f√ºr Vision)
            system_prompt: Optional - System-Prompt (√ºberschreibt Default)
            max_tokens: Maximale Antwort-L√§nge
            temperature: Kreativit√§t (0.0-1.0)
            
        Returns:
            Die generierte Antwort (nur Text, kein Metadaten)
        """
        
        # Baue den finalen Prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:"
        else:
            full_prompt = prompt
        
        # Baue llama-cli Befehl
        cmd = [
            str(self.llama_cli),
            "-m", str(self.model),
            "-p", full_prompt,
            "-n", str(max_tokens),
            "-ngl", str(config.N_GPU_LAYERS),
            "-c", str(config.CONTEXT_SIZE),
            "--temp", str(temperature),
            "--no-display-prompt",  # Zeige nicht den Input-Prompt
        ]
        
        # F√ºge mmproj hinzu wenn Bild UND mmproj vorhanden
        if image_path and self.mmproj.exists():
            cmd.extend(["--mmproj", str(self.mmproj)])
            cmd.extend(["--image", str(image_path)])
        
        # F√ºhre aus
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,  # Max 2 Minuten
                check=True
            )
            
            # Parse Output
            output = result.stdout.strip()
            
            # Extrahiere nur die Antwort (ohne llama.cpp Metadaten)
            response = self._extract_response(output)
            
            return response
            
        except subprocess.TimeoutExpired:
            return "‚ùå ERROR: Timeout (>2 Min) - Modell antwortet nicht!"
        except subprocess.CalledProcessError as e:
            return f"‚ùå ERROR: llama-cli Fehler:\n{e.stderr}"
        except Exception as e:
            return f"‚ùå ERROR: {str(e)}"
    
    def _extract_response(self, output: str) -> str:
        """
        Extrahiert die reine Antwort aus llama.cpp Output
        
        llama.cpp gibt zus√§tzliche Infos aus (Ladezeit, t/s etc.)
        Wir wollen nur die Textantwort.
        """
        
        # Entferne llama.cpp Metadaten (beginnen mit "llama_" oder "[")
        lines = output.split('\n')
        response_lines = []
        
        for line in lines:
            # √úberspringe Metadaten-Zeilen
            if line.startswith('llama_'):
                continue
            if line.startswith('[') and 'Prompt:' in line:
                continue
            if line.startswith('ggml_'):
                continue
            if not line.strip():
                continue
                
            response_lines.append(line)
        
        response = '\n'.join(response_lines).strip()
        
        # Falls leer, gib original zur√ºck (als Fallback)
        if not response:
            response = output
        
        return response
    
    def enhance_prompt(self, text_prompt: str) -> str:
        """
        Verbessert einen Text-Prompt (Text ‚Üí Enhanced Text)
        
        Args:
            text_prompt: Der zu verbessernde Prompt
            
        Returns:
            Verbesserter Stable Diffusion Prompt
        """
        return self.generate(
            prompt=text_prompt,
            system_prompt=config.SYSTEM_PROMPT_ENHANCE,
            max_tokens=300,
            temperature=0.7
        )
    
    def describe_image(self, image_path: Path) -> str:
        """
        Beschreibt ein Bild und erstellt einen SD-Prompt (Image ‚Üí Prompt)
        
        Args:
            image_path: Pfad zum Bild
            
        Returns:
            Stable Diffusion Prompt basierend auf dem Bild
        """
        if not image_path.exists():
            return f"‚ùå ERROR: Bild nicht gefunden: {image_path}"
        
        return self.generate(
            prompt="Describe this image and create a Stable Diffusion prompt.",
            image_path=image_path,
            system_prompt=config.SYSTEM_PROMPT_DESCRIBE,
            max_tokens=400,
            temperature=0.6
        )
    
    def multimodal_enhance(self, text_prompt: str, image_path: Path) -> str:
        """
        Kombiniert Bild + Text ‚Üí Enhanced Prompt (Multimodal)
        
        Args:
            text_prompt: User-Text
            image_path: Referenz-Bild
            
        Returns:
            Enhanced Prompt basierend auf Bild UND Text
        """
        if not image_path.exists():
            return f"‚ùå ERROR: Bild nicht gefunden: {image_path}"
        
        combined_system = f"""{config.SYSTEM_PROMPT_DESCRIBE}

The user provides both a text prompt AND a reference image.
Your task: Combine both to create an enhanced Stable Diffusion prompt.
- Incorporate visual elements from the image
- Enhance the user's text intent
- Create a cohesive, detailed prompt
"""
        
        return self.generate(
            prompt=f"Text: {text_prompt}",
            image_path=image_path,
            system_prompt=combined_system,
            max_tokens=500,
            temperature=0.7
        )


# Test wenn direkt ausgef√ºhrt
if __name__ == "__main__":
    print("üß™ LlamaWrapper Test")
    print("=" * 60)
    
    wrapper = LlamaWrapper()
    
    # Test 1: Text Enhancement
    print("\n[Test 1] Text Enhancement")
    result = wrapper.enhance_prompt("a cat")
    print(f"Input:  'a cat'")
    print(f"Output: {result[:200]}...")
    
    print("\n‚úÖ LlamaWrapper funktioniert!")
