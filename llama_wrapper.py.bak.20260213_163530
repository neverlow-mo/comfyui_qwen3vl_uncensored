import os
import re
import subprocess
import time
from pathlib import Path

from . import config


class LlamaWrapper:
    """
    llama.cpp wrapper (llama-cli via subprocess)

    Reality check:
    - Some llama-cli builds output heavily (or entirely) to stderr.
    - Some builds may write to a TTY directly (so PIPE capture becomes empty).
    Fix:
    - Try PIPE capture first.
    - If empty, fall back to PTY capture.
    """

    def __init__(self):
        self.llama_cli: Path = config.LLAMA_CLI
        self.model: Path = config.MODEL_PATH
        self.mmproj: Path = config.MMPROJ_PATH

        if not self.llama_cli.exists():
            raise FileNotFoundError("llama-cli nicht gefunden")
        if not self.model.exists():
            raise FileNotFoundError(f"Modell nicht gefunden: {self.model}")

    def _build_cmd(self, full_prompt: str, image_path: str | None = None) -> list[str]:
        cmd = [
            str(self.llama_cli),
            "-m", str(self.model),
            "-st",              # single-turn
            "-p", full_prompt,  # prompt
            "-c", "8192",
            "-ngl", "99",
        ]
        if image_path and self.mmproj.exists():
            cmd.extend(["--mmproj", str(self.mmproj), "--image", str(image_path)])
        return cmd

    def _run_pipes(self, cmd: list[str]) -> str:
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=180,
            check=True,
        )
        out = (result.stdout or "").strip("\n")
        err = (result.stderr or "").strip("\n")
        combined = "\n".join([s for s in (out, err) if s]).strip()
        return combined

    def _run_pty(self, cmd: list[str]) -> str:
        """
        Run the command attached to a pseudo-terminal so we also capture
        output that is written to TTY directly.
        """
        import pty
        import select

        master_fd, slave_fd = pty.openpty()
        try:
            p = subprocess.Popen(
                cmd,
                stdin=slave_fd,
                stdout=slave_fd,
                stderr=slave_fd,
                text=False,  # we decode manually
                close_fds=True,
            )

            os.close(slave_fd)

            chunks: list[bytes] = []
            start = time.time()
            timeout = 180

            while True:
                if time.time() - start > timeout:
                    p.kill()
                    raise subprocess.TimeoutExpired(cmd, timeout)

                r, _, _ = select.select([master_fd], [], [], 0.2)
                if r:
                    try:
                        data = os.read(master_fd, 65536)
                    except OSError:
                        data = b""
                    if data:
                        chunks.append(data)

                # process ended?
                if p.poll() is not None:
                    # drain remaining
                    while True:
                        r, _, _ = select.select([master_fd], [], [], 0.05)
                        if not r:
                            break
                        try:
                            data = os.read(master_fd, 65536)
                        except OSError:
                            break
                        if not data:
                            break
                        chunks.append(data)
                    break

            raw = b"".join(chunks)
            text = raw.decode("utf-8", errors="replace")
            return text.strip()
        finally:
            try:
                os.close(master_fd)
            except OSError:
                pass
            try:
                os.close(slave_fd)
            except OSError:
                pass

    def _extract_answer(self, text: str) -> str:
        if not text:
            return ""

        lines = text.splitlines()
        cleaned: list[str] = []
        in_commands_block = False

        stop_patterns = [
            re.compile(r"^\s*\[\s*Prompt\s*:", re.IGNORECASE),
            re.compile(r"^\s*Exiting\.\.\.\s*$", re.IGNORECASE),
        ]

        for raw in lines:
            line = raw.rstrip("\r")
            s = line.strip()

            if any(p.search(line) for p in stop_patterns):
                break

            if not s:
                continue

            # drop banners / headers / logs
            if "▄▄" in line or "▀▀" in line or "████" in line:
                continue
            if s.startswith("llama_") or s.startswith("ggml_"):
                continue
            if "compute capability" in s or "GeForce" in s:
                continue
            low = s.lower()
            if low.startswith("build") or low.startswith("model") or low.startswith("modalities"):
                continue

            # skip available commands block
            if low == "available commands:":
                in_commands_block = True
                continue
            if in_commands_block:
                if s.startswith("> "):
                    in_commands_block = False
                else:
                    continue

            # skip echoed prompt lines (single-turn often prints them)
            if s.startswith("> "):
                continue

            # also drop our instruction prompt lines (heuristic):
            if s.startswith("You are a helpful assistant") or s.startswith("Rewrite and enhance") or s.startswith("User prompt:") or s.startswith("Output ONLY"):
                continue

            cleaned.append(line)

        return "\n".join(cleaned).strip()

    def _run_llama(self, full_prompt: str, image_path: str | None = None) -> str:
        cmd = self._build_cmd(full_prompt, image_path=image_path)

        combined = ""
        try:
            combined = self._run_pipes(cmd)
        except subprocess.CalledProcessError as e:
            # include whatever we got
            err = (e.stderr or "").strip()
            out = (e.stdout or "").strip()
            combined = "\n".join([s for s in (out, err) if s]).strip()
            if not combined:
                return "❌ ERROR: llama-cli Fehler (kein Output abgefangen)."
        except subprocess.TimeoutExpired:
            return "❌ ERROR: llama-cli Timeout."

        # If pipes capture is empty but we clearly saw output in terminal before,
        # this likely means llama-cli wrote to TTY. Use PTY fallback.
        if not combined:
            try:
                combined = self._run_pty(cmd)
            except subprocess.TimeoutExpired:
                return "❌ ERROR: llama-cli Timeout (PTY)."
            except Exception as e:
                return f"❌ ERROR: PTY capture failed: {e}"

        return self._extract_answer(combined)

    # Public API expected by qwen3vl_node.py
    def enhance_prompt(self, text: str) -> str:
        full_prompt = (
            "You are a helpful assistant that writes high-quality, detailed prompts for image generation.\n"
            "Rewrite and enhance the user prompt into a single, vivid image-generation prompt.\n"
            f"User prompt: {text}\n"
            "Output ONLY the final enhanced prompt."
        )
        return self._run_llama(full_prompt)

    def describe_image(self, image_path: str) -> str:
        full_prompt = (
            "You are a vision assistant. Describe the image in rich, concrete detail.\n"
            "Output ONLY the description."
        )
        return self._run_llama(full_prompt, image_path=image_path)

    def multimodal_enhance(self, text: str, image_path: str) -> str:
        full_prompt = (
            "You are a helpful multimodal assistant.\n"
            "Given the image and the user text, produce a single enhanced image-generation prompt.\n"
            f"User text: {text}\n"
            "Output ONLY the final enhanced prompt."
        )
        return self._run_llama(full_prompt, image_path=image_path)
